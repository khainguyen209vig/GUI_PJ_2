import streamlit as st
import pandas as pd
import pickle
import base64
import numpy as np
from surprise import KNNBaseline
import gzip

# C√°c Def c·∫ßn thi·∫øt
def get_top_3_similar_products(ma_sp, sp, corr_mat):
    # T√¨m c√°c s·∫£n ph·∫©m chung h·ªá, ƒë·∫£m b·∫£o l√† danh s√°ch ph·∫≥ng (flat list)
    sp_cung_he = sp[sp['ma_san_pham'] == ma_sp]['san_pham_cung_he'].tolist()[0]  # L·∫•y danh s√°ch s·∫£n ph·∫©m t∆∞∆°ng ·ª©ng

    # T√¨m ki·∫øm d√≤ng c√≥ ma_san_pham = ma_sp v√† tr·∫£ v·ªÅ index
    index_can_tim = sp[sp['ma_san_pham'] == ma_sp].index.tolist()[0]

    # T·∫°o result_df t·ª´ ma tr·∫≠n t∆∞∆°ng t·ª±
    result_df = pd.DataFrame(corr_mat.iloc[index_can_tim])
    result_df.rename(columns={result_df.columns[0]: 'similarity'}, inplace=True)

    # T·∫°o √°nh x·∫° ma_san_pham v√† diem_trung_binh
    mapping = dict(enumerate(sp['ma_san_pham']))
    diem_trung_binh_mapping = dict(enumerate(sp['diem_trung_binh']))
    sp_cung_he_mapping = dict(enumerate(sp['san_pham_cung_he']))
    gia_ban_mapping = dict(enumerate(sp['gia_ban']))

    # √Ånh x·∫° ma_san_pham v√† diem_trung_binh v√†o result_df theo index
    result_df['ma_san_pham'] = result_df.index.map(mapping)
    result_df['diem_trung_binh'] = result_df.index.map(diem_trung_binh_mapping)
    result_df['san_pham_cung_he'] = result_df.index.map(sp_cung_he_mapping).astype('str')
    result_df['gia_ban'] = result_df.index.map(gia_ban_mapping)

    # S·∫Øp x·∫øp theo 'san_pham_cung_he', 'diem_trung_binh' (gi·∫£m d·∫ßn), v√† 'gia_ban' (tƒÉng d·∫ßn)
    result_df = result_df.sort_values(by=['san_pham_cung_he', 'diem_trung_binh', 'gia_ban'], ascending=[True, False, True])

    # L·ªçc b·ªè c√°c d√≤ng tr√πng 'san_pham_cung_he', gi·ªØ l·∫°i d√≤ng ƒë·∫ßu ti√™n trong m·ªói nh√≥m
    result_df = result_df.drop_duplicates(subset='san_pham_cung_he', keep='first')

    # S·ª≠ d·ª•ng apply ƒë·ªÉ ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa 'ma_san_pham' trong 'san_pham_cung_he'
    top3_similarity = result_df[(result_df['diem_trung_binh'] >= 4) & (~result_df['ma_san_pham'].apply(lambda x: str(x) in sp_cung_he)) & (result_df['similarity'] < 0.9)].sort_values(by='similarity', ascending=False).head(3)

    # Th√™m s·∫£n ph·∫©m c·ªßa ma_sp v√†o k·∫øt qu·∫£
    top3_similarity = pd.concat([
        sp[sp['ma_san_pham'] == ma_sp][['ma_san_pham']],
        top3_similarity
    ], ignore_index=True)

    # Tr·∫£ v·ªÅ k·∫øt qu·∫£ sau khi merge v·ªõi th√¥ng tin s·∫£n ph·∫©m
    result = pd.merge(top3_similarity, sp[['ma_san_pham', 'ten_san_pham', 'mo_ta', 'gia_goc', 'phan_loai']], on='ma_san_pham', how='left')

    return result
def recommend_userid(userId,algorithm,df,num):
    # df_select = df[(df['ma_khach_hang'] == userId) & (df['so_sao'] >=num)]
    # df_select = df_select.set_index('ma_san_pham')
    # df_select.head(df_select.shape[0])
    df_score = df[["ma_san_pham",'diem_trung_binh','ten_san_pham','gia_ban','san_pham_cung_danh_gia']]
    df_score['EstimateScore'] = df_score['ma_san_pham'].apply(lambda x: algorithm.predict(userId, x).est) # est: get EstimateScore
    df_score = df_score.sort_values(by=['EstimateScore'], ascending=False)
    df_score = df_score.drop_duplicates(subset=['ma_san_pham','ten_san_pham'])
    idx_min = df_score.groupby('san_pham_cung_danh_gia')['gia_ban'].idxmin()
    df_final = df_score.loc[idx_min, ['san_pham_cung_danh_gia','ma_san_pham', 'gia_ban', 'ten_san_pham','diem_trung_binh','EstimateScore']].sort_values(by='EstimateScore', ascending=False)
    f=df_final[0:num]
    return f

#Streamlit
#Main_bacground
st.markdown(
    """
    <style>
    .stApp {
        background-image: url('https://i.pinimg.com/736x/fe/3d/89/fe3d893b8315d7b0745ce62846f6d4f7.jpg  ');
        background-size: 70%;
        background-position: center;
    }
    </style>
    """, 
    unsafe_allow_html=True
)
#Sidebar background
#·∫¢nh background sidebar
def get_base64_of_bin_file(bin_file):
    with open(bin_file, 'rb') as f:
        data = f.read()
    return base64.b64encode(data).decode()

background_image_path = "back_sidebar.jpg"
background_image = get_base64_of_bin_file(background_image_path)

st.markdown(f"""
<style>
[data-testid="stSidebar"] {{
    background-image: url("data:image/jpeg;base64,{background_image}");
    background-size: 80%;
    background-position: right bottom;
    background-repeat: no-repeat;
}}
</style>
""", unsafe_allow_html=True)

st.markdown("""
<style>
    [data-testid=stSidebar] {
        background-color: #C0C0C0;
    }
</style>
""", unsafe_allow_html=True)

#Side bar
menu = ["T·ªïng Quan", "ƒê·ªÅ xu·∫•t d·ª±a tr√™n m√£ s·∫£n ph·∫©m", "ƒê·ªÅ xu·∫•t d·ª±a tr√™n UserID"]
choice = st.sidebar.selectbox('#### **Danh M·ª•c**', menu)
st.sidebar.write("""#### *Th√†nh vi√™n th·ª±c hi·ªán:*
                 Nguy·ªÖn Quang Kh·∫£i 
    Trang Th∆∞ ƒê√¨nh """)
st.sidebar.write("""#### *Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n:*
                Khu·∫•t Th√πy Ph∆∞∆°ng""")
st.sidebar.write("""#### Th·ªùi gian th·ª±c hi·ªán: 12/2024""")


st.title("Data Science Project:")
st.write("## ***Recommender System***")

#Chi ti·∫øt t·ª´ng page
if choice == 'T·ªïng Quan':
    # CSS t√πy ch·ªânh
    st.markdown(
        """
        <style>
        .main-title {
            text-align: center;
            font-size: 3rem; /* TƒÉng k√≠ch th∆∞·ªõc ti√™u ƒë·ªÅ ch√≠nh */
            font-weight: bold;
            color: #FF6F61;
        }
        .sub-title {
            text-align: center;
            font-size: 1.8rem; /* TƒÉng k√≠ch th∆∞·ªõc ti√™u ƒë·ªÅ ph·ª• */
            color: #666;
            margin-bottom: 20px;
        }
        .section-title {
            font-size: 2rem; /* TƒÉng k√≠ch th∆∞·ªõc ti√™u ƒë·ªÅ c√°c ph·∫ßn */
            color: #333;
            margin-top: 20px;
            margin-bottom: 10px;
            border-bottom: 2px solid #FF6F61;
        }
        .content-text {
            font-size: 1.2rem; /* TƒÉng k√≠ch th∆∞·ªõc n·ªôi dung vƒÉn b·∫£n */
            line-height: 1.8; /* TƒÉng kho·∫£ng c√°ch d√≤ng ƒë·ªÉ d·ªÖ ƒë·ªçc h∆°n */
            color: #000000;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    # Header ch√≠nh
    st.markdown('<h1 class="main-title">üíÑ HASAKI.VN Recommender System</h1>', unsafe_allow_html=True)
    st.markdown('<h2 class="sub-title">TƒÉng c∆∞·ªùng tr·∫£i nghi·ªám mua s·∫Øm v·ªõi h·ªá th·ªëng g·ª£i √Ω c√° nh√¢n h√≥a</h2>', unsafe_allow_html=True)

    # H√¨nh ·∫£nh minh h·ªça
    st.image(
        "hasaki_logo.jpg", 
        caption="H·ªá th·ªëng m·ªπ ph·∫©m v√† chƒÉm s√≥c s·∫Øc ƒë·∫πp HASAKI.VN", 
        use_container_width=True,
    )

    # Tabs cho t·ª´ng ph·∫ßn n·ªôi dung
    tab1, tab2, tab3 = st.tabs(["Gi·ªõi thi·ªáu", "M·ª•c ti√™u kinh doanh", "T√¨m hi·ªÉu th√™m"])

    with tab1:
        st.markdown('<h2 class="section-title">Gi·ªõi thi·ªáu v·ªÅ Recommender System</h2>', unsafe_allow_html=True)
        st.markdown(
            """
            <div class="content-text">
            Recommender System (H·ªá th·ªëng g·ª£i √Ω) l√† m·ªôt h·ªá th·ªëng l·ªçc th√¥ng tin nh·∫±m d·ª± ƒëo√°n c√°c m·ª•c m√† ng∆∞·ªùi d√πng c√≥ th·ªÉ quan t√¢m. 
            Ch√∫ng th∆∞·ªùng ƒë∆∞·ª£c √°p d·ª•ng trong c√°c ng√†nh th∆∞∆°ng m·∫°i nh∆∞:
            <ul>
                <li>üéµ G·ª£i √Ω danh s√°ch ph√°t nh·∫°c/video: Netflix, YouTube, Spotify.</li>
                <li>üõçÔ∏è ƒê·ªÅ xu·∫•t s·∫£n ph·∫©m: Amazon, Tiki.</li>
                <li>üìö G·ª£i √Ω kh√≥a h·ªçc: Coursera, Udemy.</li>
                <li>üì± ƒê·ªÅ xu·∫•t n·ªôi dung: Facebook, Twitter.</li>
            </ul>
            </div>
            """,
            unsafe_allow_html=True,
        )

    with tab2:
        st.markdown('<h2 class="section-title">M·ª•c ti√™u kinh doanh t·∫°i HASAKI.VN</h2>', unsafe_allow_html=True)
        st.markdown(
            """
            <div class="content-text">
            **HASAKI.VN** l√† h·ªá th·ªëng c·ª≠a h√†ng m·ªπ ph·∫©m ch√≠nh h√£ng v√† d·ªãch v·ª• chƒÉm s√≥c s·∫Øc ƒë·∫πp chuy√™n s√¢u v·ªõi h·ªá th·ªëng tr·∫£i d√†i tr√™n to√†n qu·ªëc. 

            M·ª•c ti√™u c·ªßa h·ªá th·ªëng g·ª£i √Ω:
            - ƒê·ªÅ xu·∫•t s·∫£n ph·∫©m m·ªπ ph·∫©m ph√π h·ª£p nh·∫•t cho t·ª´ng kh√°ch h√†ng.
            - TƒÉng tr·∫£i nghi·ªám ng∆∞·ªùi d√πng.
            - T·ªëi ∆∞u h√≥a doanh thu th√¥ng qua c√°c g·ª£i √Ω c√° nh√¢n h√≥a.
            </div>
            """,
            unsafe_allow_html=True,
        )
        with st.expander("Chi ti·∫øt v·ªÅ HASAKI.VN"):
            st.markdown(
                """
                <div class="content-container">
                <div class="content-text">
                HASAKI l√† ƒë·ªëi t√°c chi·∫øn l∆∞·ª£c t·∫°i Vi·ªát Nam c·ªßa nhi·ªÅu th∆∞∆°ng hi·ªáu l·ªõn. 
                Kh√°ch h√†ng c√≥ th·ªÉ:
                <ul>
                    <li>T√¨m ki·∫øm v√† l·ª±a ch·ªçn s·∫£n ph·∫©m ph√π h·ª£p.</li>
                    <li>Xem c√°c ƒë√°nh gi√° v√† nh·∫≠n x√©t t·ª´ ng∆∞·ªùi d√πng kh√°c.</li>
                    <li>ƒê·∫∑t mua s·∫£n ph·∫©m m·ªôt c√°ch nhanh ch√≥ng v√† ti·ªán l·ª£i.</li>
                </ul>
                </div>
                </div>
                """,
                unsafe_allow_html=True, 
            )

    with tab3:
        st.markdown('<h2 class="section-title">T√¨m hi·ªÉu th√™m v·ªÅ H·ªá th·ªëng G·ª£i √Ω</h2>', unsafe_allow_html=True)
        st.markdown(
            """
            <div class="content-text">
            C√≥ hai lo·∫°i Recommender System ph·ªï bi·∫øn nh·∫•t:
            <ul>
                <li><b>Collaborative Filtering (CF):</b> S·ª≠ d·ª•ng th√¥ng tin t·ª´ c·ªông ƒë·ªìng ng∆∞·ªùi d√πng ƒë·ªÉ ƒë·ªÅ xu·∫•t.</li>
                <li><b>Content-Based Filtering:</b> D·ª±a v√†o ƒë·∫∑c ƒëi·ªÉm c·ªßa s·∫£n ph·∫©m v√† s·ªü th√≠ch c·ªßa ng∆∞·ªùi d√πng.</li>
            </ul>
            </div>
            """,
            unsafe_allow_html=True,
        )
        st.info("üéØ HASAKI.VN ƒëang t√≠ch h·ª£p c·∫£ hai ph∆∞∆°ng ph√°p n√†y ƒë·ªÉ mang l·∫°i g·ª£i √Ω ch√≠nh x√°c nh·∫•t cho b·∫°n!")


    st.write("---")
    st.success("### üëâ H√£y kh√°m ph√° c√°ch h·ªá th·ªëng g·ª£i √Ω c·ªßa HASAKI.VN c√≥ th·ªÉ n√¢ng cao tr·∫£i nghi·ªám c·ªßa b·∫°n!")
elif choice == 'ƒê·ªÅ xu·∫•t d·ª±a tr√™n m√£ s·∫£n ph·∫©m':
    st.subheader('ƒê·ªÅ xu·∫•t d·ª±a tr√™n m√£ s·∫£n ph·∫©m')
    #ƒê·ªçc file
    sp = sp = pd.read_csv('All_San_pham_clean.csv',sep=';')
    # cosine_matrix = np.load('cosine_similarity.npy', allow_pickle=True)
    with gzip.GzipFile('cosine_similarity.npy.gz', 'rb') as f:
        cosine_matrix = np.load(f)
    df_cosine = pd.DataFrame(cosine_matrix)
    sp_sample = sp.sample(n=15, random_state=40)

    # T·∫°o select tra theo 2 h∆∞·ªõng
    search_mode = st.radio("B·∫°n mu·ªën tra c·ª©u theo:", ["T√™n s·∫£n ph·∫©m", "M√£ s·∫£n ph·∫©m"])

    if search_mode == "T√™n s·∫£n ph·∫©m":
        # K·∫øt h·ª£p text_input v√† selectbox
        user_input = st.text_input("Nh·∫≠p t√™n s·∫£n ph·∫©m:")
        selected_name = st.selectbox(
            "Ho·∫∑c ch·ªçn t·ª´ g·ª£i √Ω:",
            options=[""] + sp_sample["ten_san_pham"].tolist(),
            help="B·∫°n c√≥ th·ªÉ nh·∫≠p t√™n t·ª± do ho·∫∑c ch·ªçn t·ª´ danh s√°ch g·ª£i √Ω."
        )
        
        # ∆Øu ti√™n gi√° tr·ªã nh·∫≠p t·ª´ text_input
        if user_input:
            search_term = user_input
        else:
            search_term = selected_name

        # Hi·ªÉn th·ªã k·∫øt qu·∫£ n·∫øu c√≥ ƒë·∫ßu v√†o
        if search_term:
           
            matching_products = sp[sp["ten_san_pham"].str.contains(search_term, case=False, na=False)]
            if not matching_products.empty:
                st.write("K·∫øt qu·∫£ t√¨m ki·∫øm:")

                product_code = matching_products["ma_san_pham"].iloc[0]
                st.write(f"**T√™n s·∫£n ph·∫©m:** {search_term}")
                st.write(f"**M√£ s·∫£n ph·∫©m t∆∞∆°ng ·ª©ng:** {product_code}")
                product_description = matching_products["mo_ta"].iloc[0]
                truncated_description = ' '.join(product_description.split()[:100])
                st.write('##### Th√¥ng tin:')
                st.write(truncated_description, '...')
                st.write('#### S·∫£n ph·∫©m g·ª£i √Ω t∆∞∆°ng t·ª±:')
                a=product_code
                result_df = get_top_3_similar_products(a, sp, df_cosine)
                # T·∫°o m·ªôt container ƒë·ªÉ ch·ª©a c√°c s·∫£n ph·∫©m
                product_container = st.container()

               
                # Duy·ªát qua t·ª´ng h√†ng trong DataFrame
                for index, row in result_df.iloc[1:].iterrows():
                    with product_container.expander(f"M√£ s·∫£n ph·∫©m: {row['ma_san_pham']} - {row['ten_san_pham']}"):
                        st.write(f"**ƒêi·ªÉm trung b√¨nh:** {row['diem_trung_binh']}")
                        st.write(f"**Gi√° b√°n:** {row['gia_ban']}")
                        st.write(f"**M√¥ t·∫£ ƒë·∫ßy ƒë·ªß:** {row['mo_ta']}")
            else:
                st.warning("Kh√¥ng t√¨m th·∫•y t√™n s·∫£n ph·∫©m ph√π h·ª£p.")
    else:
        # K·∫øt h·ª£p text_input v√† selectbox
        user_input = st.text_input("Nh·∫≠p m√£ s·∫£n ph·∫©m:")
        selected_code = st.selectbox(
            "Ho·∫∑c ch·ªçn t·ª´ g·ª£i √Ω:",
            options=[""] + sp_sample["ma_san_pham"].astype(str).tolist(),
            help="B·∫°n c√≥ th·ªÉ nh·∫≠p m√£ t·ª± do ho·∫∑c ch·ªçn t·ª´ danh s√°ch g·ª£i √Ω."
        )
        
        # ∆Øu ti√™n gi√° tr·ªã nh·∫≠p t·ª´ text_input
        search_term = user_input if user_input else selected_code

        # Hi·ªÉn th·ªã k·∫øt qu·∫£ n·∫øu c√≥ ƒë·∫ßu v√†o
        if search_term:
            # Ensure ma_san_pham is string type
            sp["ma_san_pham"] = sp["ma_san_pham"].astype(str)
            # Search
            matching_products = sp[sp["ma_san_pham"].str.contains(search_term, case=False, na=False)]
            if not matching_products.empty:
                # Get the first product if there's a match
                product_name = matching_products["ten_san_pham"].iloc[0]
                st.write("K·∫øt qu·∫£ t√¨m ki·∫øm:")
                st.write(f"**T√™n s·∫£n ph·∫©m t∆∞∆°ng ·ª©ng:** {product_name}")
                st.write(f"**M√£ s·∫£n ph·∫©m:** {search_term}")
                product_description = matching_products["mo_ta"].iloc[0]
                truncated_description = ' '.join(product_description.split()[:100])
                st.write('**Th√¥ng tin:**')
                st.write(truncated_description, '...')
                st.write('#### S·∫£n ph·∫©m g·ª£i √Ω t∆∞∆°ng t·ª±:')
                a=search_term
                result_df = get_top_3_similar_products(a, sp, df_cosine)
                # T·∫°o m·ªôt container ƒë·ªÉ ch·ª©a c√°c s·∫£n ph·∫©m
                product_container = st.container()

                # T√πy ch·ªânh ƒë·ªô d√†i m√¥ t·∫£ ng·∫Øn
                # max_chars_short_description = 100
               
                # Duy·ªát qua t·ª´ng h√†ng trong DataFrame
                for index, row in result_df.iloc[1:].iterrows():
                    with product_container.expander(f"M√£ s·∫£n ph·∫©m: {row['ma_san_pham']} - {row['ten_san_pham']}"):
                        st.write(f"**ƒêi·ªÉm trung b√¨nh:** {row['diem_trung_binh']}")
                        st.write(f"**Gi√° b√°n:** {row['gia_ban']}")
                        st.write(f"**M√¥ t·∫£ ƒë·∫ßy ƒë·ªß:** {row['mo_ta']}")
                        # # Hi·ªÉn th·ªã m√¥ t·∫£ ng·∫Øn v·ªõi n√∫t "Xem th√™m"
                        # mo_ta_ngan = row['mo_ta'][:max_chars_short_description]
                        # if len(row['mo_ta']) > max_chars_short_description:
                        #     mo_ta_ngan += "..."
                        # st.write(f"**M√¥ t·∫£:** {mo_ta_ngan}")

                        # # Hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß m√¥ t·∫£ khi click v√†o n√∫t "Xem th√™m"
                        # if st.button("Xem th√™m", key=f"button_{index}"):
                            

            else:
                st.warning("Kh√¥ng t√¨m th·∫•y m√£ s·∫£n ph·∫©m ph√π h·ª£p.")
elif choice == 'ƒê·ªÅ xu·∫•t d·ª±a tr√™n UserID':
    df = pd.read_csv('df_data_user.csv')

    with gzip.open('knn_model_compressed.pkl.gz', 'rb') as f:
        algorithm = pickle.load(f)
    
    st.subheader('ƒê·ªÅ xu·∫•t d·ª±a tr√™n UserID')
    user_input = st.text_input("Nh·∫≠p m√£ kh√°ch h√†ng c·ªßa b·∫°n:")
    

    if id:
        # Hi·ªán l·ªãch s·ª≠ mua h√†ng
        try:
            id = int(user_input)
            df_mua_hang=df[df['ma_khach_hang'] == id]
            ten_khach_hang=df_mua_hang['ho_ten_x'].iloc[0]
            st.write("K·∫øt qu·∫£ t√¨m ki·∫øm:")
            st.write(f"**T√™n kh√°ch h√†ng:** {ten_khach_hang}")
            st.write(f"**M√£ kh√°c h√†ng:** {id}")
            st.write('**L·ªãch s·ª≠ mua h√†ng:**')
            his_b=df_mua_hang[['ma_san_pham', 'ten_san_pham', 'gia_ban', 'diem_trung_binh']]
            st.table(his_b)
            
            # ƒê·ªÅ xu·∫•t mua h√†ng:
            result_id= recommend_userid(id,algorithm,df,5)
            x = result_id[['ma_san_pham', 'ten_san_pham', 'gia_ban', 'diem_trung_binh', 'EstimateScore']]
            st.write('#### S·∫£n ph·∫©m g·ª£i √Ω cho kh√°ch h√†ng:')
            st.table(x)

        except Exception:
            st.error("M√£ kh√°ch h√†ng kh√¥ng h·ª£p l·ªá ho·∫∑c kh√¥ng t·ªìn t·∫°i. Vui l√≤ng nh·∫≠p ID l√† m·ªôt s·ªë nguy√™n.")
    else:
        st.warning("B·∫°n ch∆∞a nh·∫≠p ID s·∫£n ph·∫©m.")




